{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c49eb7b-9529-4442-adee-d138d682f016",
   "metadata": {},
   "source": [
    "# REQUIRED LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf14ae7-b7cb-4bec-8c8f-b54446dcded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting peft==0.14.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers==4.47.1\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting trl==0.13.0\n",
      "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate==1.2.1\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (1.8.1)\n",
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (2.3.0a0+git96dd291)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from peft==0.14.0) (4.66.4)\n",
      "Collecting safetensors (from peft==0.14.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.25.0 (from peft==0.14.0)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (3.13.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.47.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from transformers==4.47.1) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.47.1)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting datasets>=2.21.0 (from trl==0.13.0)\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl==0.13.0)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.17 (from peft==0.14.0)\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting numpy>=1.17 (from peft==0.14.0)\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from tensorboardX) (3.20.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl==0.13.0)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (0.3.7)\n",
      "Collecting xxhash (from datasets>=2.21.0->trl==0.13.0)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl==0.13.0)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl==0.13.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from datasets>=2.21.0->trl==0.13.0) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from requests->transformers==4.47.1) (2024.7.4)\n",
      "Collecting sympy<=1.12.1 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (3.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl==0.13.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from rich->trl==0.13.0) (2.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (24.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl==0.13.0) (4.0.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl==0.13.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl==0.13.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from sympy<=1.12.1->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (2.1.5)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, sympy, safetensors, regex, pyarrow, numpy, mdurl, dill, tensorboardX, pandas, multiprocess, markdown-it-py, huggingface-hub, tokenizers, rich, accelerate, transformers, datasets, trl, peft\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.2\n",
      "    Uninstalling numpy-1.21.2:\n",
      "      Successfully uninstalled numpy-1.21.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.55.2 requires numpy<1.23,>=1.18, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.2.1 datasets-3.5.0 dill-0.3.8 huggingface-hub-0.30.1 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.16 numpy-1.24.4 pandas-2.2.3 peft-0.14.0 pyarrow-19.0.1 pytz-2025.2 regex-2024.11.6 rich-14.0.0 safetensors-0.5.3 sympy-1.12.1 tensorboardX-2.6.2.2 tokenizers-0.21.1 transformers-4.47.1 trl-0.13.0 tzdata-2025.2 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas peft==0.14.0 transformers==4.47.1 trl==0.13.0 accelerate==1.2.1 scipy tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05613f66-1068-4e1c-9e2f-86d6f1e87dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bitsandbytes'...\n",
      "remote: Enumerating objects: 8235, done.\u001b[K\n",
      "remote: Counting objects: 100% (2675/2675), done.\u001b[K\n",
      "remote: Compressing objects: 100% (315/315), done.\u001b[K\n",
      "remote: Total 8235 (delta 2513), reused 2360 (delta 2360), pack-reused 5560 (from 2)\u001b[K\n",
      "Receiving objects: 100% (8235/8235), 2.49 MiB | 8.40 MiB/s, done.\n",
      "Resolving deltas: 100% (5604/5604), done.\n",
      "Already on 'rocm_enabled_multi_backend'\n",
      "Your branch is up to date with 'origin/rocm_enabled_multi_backend'.\n",
      "Requirement already satisfied: setuptools>=63 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from -r requirements-dev.txt (line 2)) (78.1.0)\n",
      "Collecting pytest~=8.3.1 (from -r requirements-dev.txt (line 3))\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting einops~=0.8.0 (from -r requirements-dev.txt (line 4))\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wheel~=0.43.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from -r requirements-dev.txt (line 5)) (0.43.0)\n",
      "Collecting lion-pytorch~=0.2.2 (from -r requirements-dev.txt (line 6))\n",
      "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
      "Collecting scipy~=1.14.0 (from -r requirements-dev.txt (line 7))\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pandas~=2.2.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from -r requirements-dev.txt (line 8)) (2.2.3)\n",
      "Collecting matplotlib~=3.9.1 (from -r requirements-dev.txt (line 9))\n",
      "  Downloading matplotlib-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pytest~=8.3.1->-r requirements-dev.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pytest~=8.3.1->-r requirements-dev.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pytest~=8.3.1->-r requirements-dev.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pytest~=8.3.1->-r requirements-dev.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pytest~=8.3.1->-r requirements-dev.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (2.3.0a0+git96dd291)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from scipy~=1.14.0->-r requirements-dev.txt (line 7)) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from pandas~=2.2.2->-r requirements-dev.txt (line 8)) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9))\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9))\n",
      "  Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9)) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.9.1->-r requirements-dev.txt (line 9))\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.2->-r requirements-dev.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: sympy<=1.12.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from sympy<=1.12.1->torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from jinja2->torch>=1.6->lion-pytorch~=0.2.2->-r requirements-dev.txt (line 6)) (2.1.5)\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
      "Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: scipy, pytest, pyparsing, kiwisolver, fonttools, einops, cycler, contourpy, matplotlib, lion-pytorch\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.8.1\n",
      "    Uninstalling scipy-1.8.1:\n",
      "      Successfully uninstalled scipy-1.8.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.3.2\n",
      "    Uninstalling pytest-7.3.2:\n",
      "      Successfully uninstalled pytest-7.3.2\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 einops-0.8.1 fonttools-4.57.0 kiwisolver-1.4.8 lion-pytorch-0.2.3 matplotlib-3.9.4 pyparsing-3.2.3 pytest-8.3.5 scipy-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Configuring bitsandbytes (Backend: hip)\n",
      "-- NO_CUBLASLT := OFF\n",
      "-- The HIP compiler identification is Clang 18.0.0\n",
      "-- Detecting HIP compiler ABI info\n",
      "-- Detecting HIP compiler ABI info - done\n",
      "-- Check for working HIP compiler: /opt/rocm/lib/llvm/bin/clang++ - skipped\n",
      "-- Detecting HIP compile features\n",
      "-- Detecting HIP compile features - done\n",
      "-- HIP Compiler: /opt/rocm/lib/llvm/bin/clang++\n",
      "-- HIP Targets: gfx90a\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "\u001b[0mhipblas VERSION: 2.2.0\u001b[0m\n",
      "\u001b[0mhiprand VERSION: 2.11.0\u001b[0m\n",
      "\u001b[0mhipsparse VERSION: 3.1.1\u001b[0m\n",
      "-- Configuring done (3.4s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /home/aac/bitsandbytes\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/bitsandbytes.dir/csrc/common.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/bitsandbytes.dir/csrc/cpu_ops.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/bitsandbytes.dir/csrc/pythonInterface.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding HIP object CMakeFiles/bitsandbytes.dir/csrc/ops.hip.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding HIP object CMakeFiles/bitsandbytes.dir/csrc/kernels.hip.o\u001b[0m\n",
      "\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      " 2858 | __global__ void kspmm_coo_very_sparse_naive(int *max_count, int *max_idx, int *offset_rowidx, int *rowidx, int *colidx, half *values, T *B, half *out, float * __restrict__ const dequant_stats, int nnz, int rowsA, int rowsB, int colsB)\u001b[0m\n",
      "      | \u001b[0;1;32m                ^\n",
      "\u001b[0m\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      "\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      "\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      "\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      "\u001b[1m/home/aac/bitsandbytes/csrc/kernels.hip:2858:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1mloop not unrolled: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Wpass-failed=transform-warning]\u001b[0m\n",
      "6 warnings generated when compiling for gfx90a.\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared library bitsandbytes/libbitsandbytes_rocm62.so\u001b[0m\n",
      "[100%] Built target bitsandbytes\n",
      "Processing /home/aac/bitsandbytes\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from bitsandbytes==0.43.3.dev0) (2.3.0a0+git96dd291)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from bitsandbytes==0.43.3.dev0) (1.24.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: sympy<=1.12.1 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.3.dev0) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from sympy<=1.12.1->torch->bitsandbytes==0.43.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.10/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.3.dev0) (2.1.5)\n",
      "Building wheels for collected packages: bitsandbytes\n",
      "  Building wheel for bitsandbytes (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bitsandbytes: filename=bitsandbytes-0.43.3.dev0-cp310-cp310-linux_x86_64.whl size=453521 sha256=5be1eb5603212543b80fe81207de06c5638131e74bd8cd31c4c3fbd857729202\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hwigb26g/wheels/b8/98/96/3e8101e4e281abb9f2906092feeff0acf542592337e01c1d25\n",
      "Successfully built bitsandbytes\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.3.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mg++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "bitsandbytes version: 0.43.3.dev\n"
     ]
    }
   ],
   "source": [
    "# Remove existing directory, clone and install bitsandbytes specifically for MI300X\n",
    "!rm -rf bitsandbytes && \\\n",
    "git clone --recurse https://github.com/ROCm/bitsandbytes.git && \\\n",
    "cd bitsandbytes && \\\n",
    "git checkout rocm_enabled_multi_backend && \\\n",
    "pip install -r requirements-dev.txt && \\\n",
    "cmake -DCOMPUTE_BACKEND=hip -DBNB_ROCM_ARCH=\"gfx90a\" -S . && \\\n",
    "make && \\\n",
    "pip install . && \\\n",
    "cd .. && \\\n",
    "python -c \"import bitsandbytes as bnb; print('bitsandbytes version:', bnb.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2bc01-ae46-4a22-9ba4-ea2a6f69536c",
   "metadata": {},
   "source": [
    "## checking if bits and bytes is installed porper ly , note : o.43 is the compactible version for the mi250 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ede0fe-d73d-4dc4-8e35-2aff42fb40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes version: 0.43.3.dev\n",
      "Installation successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(\"bitsandbytes version:\", bnb.__version__)\n",
    "    print(\"Installation successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Error importing bitsandbytes:\", e)\n",
    "    print(\"bitsandbytes is not installed correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a4e89-dd98-4102-bf77-01bf61a12947",
   "metadata": {},
   "source": [
    "# Huggingface api auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815f9ab-a717-46f9-900e-ca5c9d9aff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: rohithreddyv1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Hugging Face Login\n",
    "# Authenticate with Hugging Face to access Llama-3.1 model\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Set your Hugging Face token directly\n",
    "# Replace 'hf_your_token_here' with your actual token\n",
    "login(token=\"hf_your_token_here\", add_to_git_credential=False)\n",
    "\n",
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d067dad-324a-40b1-af29-ff142b6c10bb",
   "metadata": {},
   "source": [
    "# Step 4: Set and Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af79f4d6-afba-47e0-b3a8-dd36e30271ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of available devices: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Specify which GPUs to use - adjust based on your hardware\n",
    "gpus = [0]  # Use [0, 1, 2, 3] for MI300x or [0] for W7900\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575351e8-4638-4fa1-8458-532cc77df374",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1774416-a626-4ecf-8b4f-02e8d4e514cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94f3ef-8729-4ce0-8656-87e47f9296c9",
   "metadata": {},
   "source": [
    "This cell imports all the required libraries:\n",
    "\n",
    "Standard libraries: os\n",
    "Machine learning: torch (PyTorch)\n",
    "Data handling: pandas\n",
    "Hugging Face libraries:\n",
    "\n",
    "datasets for working with training datasets\n",
    "transformers for accessing models, tokenizers, and training utilities\n",
    "peft for Parameter-Efficient Fine-Tuning\n",
    "trl for Transformer Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac08e60-f31f-4bf5-9388-0c7d8a23be27",
   "metadata": {},
   "source": [
    "# GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ae0808-bf61-477c-8f0d-cac2e82409be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch detected number of available devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Set and verify GPU availability\n",
    "gpus = [0]  # For single GPU; use [0, 1, 2, 3] for MI300X with multiple GPUs\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c0ef6-b959-4736-93ee-861a8d8a6197",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Sets up which GPU(s) to use for training\n",
    "Verifies GPU availability through PyTorch\n",
    "Allows configuration for multi-GPU setups (like MI250X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492cb57-5149-4c3c-ab33-65d6caec72c7",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ed0f19-b383-43d1-991b-da1f70ecf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "new_model_name = \"rohith-llama-3.1-8B-finetuned\"\n",
    "\n",
    "# Configure 4-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f8ae9-41a8-44c6-9104-cb3415c2003e",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Specifies the base model (LLaMA 3.1 8B) to fine-tune\n",
    "Names the fine-tuned model\n",
    "Sets up 4-bit quantization to reduce GPU memory usage\n",
    "\n",
    "Uses NF4 (normalized float 4) quantization\n",
    "Computes in float16 for better precision\n",
    "Enables double quantization for additional memory savings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca7d2d-7979-4307-8658-9f7ea8d94aff",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c82206-46d5-433a-8765-2457d5d33bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully!\n",
      "Loading model... (this may take a few minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [03:41<00:00, 55.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda:0!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "# Load model with quantization\n",
    "print(\"Loading model... (this may take a few minutes)\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to optimize for fine-tuning\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "print(f\"Model loaded successfully on {base_model.device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaf632-9bf8-43bc-9e1c-4f33ebd66526",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Loads the LLaMA tokenizer from Hugging Face\n",
    "\n",
    "Sets the padding token to be the same as the end-of-sequence token\n",
    "Configures right padding (important for causal language models)\n",
    "Uses fast tokenizers for better performance\n",
    "\n",
    "\n",
    "Loads the LLaMA 3.1 model with 4-bit quantization\n",
    "\n",
    "Uses device_map=\"auto\" for optimal GPU placement\n",
    "Disables KV caching to save memory during training\n",
    "Sets tensor parallelism to 1 (using single-tensor operations)\n",
    "Confirms the device(s) the model is loaded on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41163b-8ea7-42d1-9984-4b8757167364",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5ba3f3-b5a2-4dc7-85ec-f0416e85e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset prepared with 137 examples\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "print(\"Preparing dataset...\")\n",
    "# Load the dataset from the text file\n",
    "with open('rohith.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Parse the JSON-like data\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Extract JSON objects from the text\n",
    "json_pattern = r'({.*?})'\n",
    "json_matches = re.findall(json_pattern, data, re.DOTALL)\n",
    "\n",
    "# Parse each JSON object\n",
    "dataset_records = []\n",
    "for json_str in json_matches:\n",
    "    try:\n",
    "        record = json.loads(json_str)\n",
    "        # Format the data for instruction fine-tuning\n",
    "        formatted_text = f\"<s>[INST] {record['instruction']} {record['input']} [/INST] {record['output']}</s>\"\n",
    "        dataset_records.append({\"text\": formatted_text})\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON: {json_str}\")\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "training_data = Dataset.from_pandas(pd.DataFrame(dataset_records))\n",
    "print(f\"Dataset prepared with {len(training_data)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922935d-4ae4-4171-87a5-9e38474d3307",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Reads the custom training data from 'rohith.txt'\n",
    "Uses regex to extract JSON objects from the text file\n",
    "Parses each JSON entry and formats it into LLaMA's instruction format:\n",
    "\n",
    "<s>[INST] instruction input [/INST] output</s>\n",
    "\n",
    "\n",
    "Creates a Hugging Face Dataset object from the formatted examples\n",
    "Reports the number of training examples extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53aae4-bef6-4cca-83a3-8fd6468df92c",
   "metadata": {},
   "source": [
    "# LoRA and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d72baa5-8839-42ab-af0c-06524e5ac6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA parameters...\n",
      "Applying LoRA to the model...\n",
      "trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n",
      "Setting up training arguments...\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA parameters\n",
    "print(\"Configuring LoRA parameters...\")\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,           # Scaling parameter\n",
    "    lora_dropout=0.1,       # Dropout probability for LoRA layers\n",
    "    r=32,                    # Rank of the low-rank matrices\n",
    "    bias=\"none\",            # Whether to train bias parameters\n",
    "    task_type=\"CAUSAL_LM\"   # The type of task\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "print(\"Applying LoRA to the model...\")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Configure training arguments\n",
    "print(\"Setting up training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_rohith_lora\",\n",
    "    num_train_epochs=50,                   # Number of training epochs\n",
    "    per_device_train_batch_size=1,        # Batch size per device during training\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",# Number of updates steps to accumulate before backward pass\n",
    "    learning_rate=4e-5,                   # Initial learning rate\n",
    "    weight_decay=0.001,                    # Weight decay to apply\n",
    "    logging_steps=1,                      # Log every X updates steps\n",
    "    save_strategy=\"epoch\",\n",
    "    max_grad_norm=0.3,# Save strategy to adopt during training\n",
    "    fp16=True,                            # Enable mixed precision training with FP16\n",
    "    logging_dir=\"./logs\",                 # Directory for storing logs\n",
    "    warmup_ratio=0.03,                    # Ratio of total training steps used for warmup\n",
    "    lr_scheduler_type=\"cosine\",           # Learning rate scheduler type\n",
    "    report_to=\"tensorboard\"               # Report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40f3d9-6a05-42a2-9a31-a5abb1720424",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Configures LoRA (Low-Rank Adaptation) parameters:\n",
    "\n",
    "lora_alpha=8: Scaling parameter for LoRA updates\n",
    "lora_dropout=0.1: Dropout rate to prevent overfitting\n",
    "r=32: Rank for low-rank matrices (higher = more capacity)\n",
    "bias=\"none\": No bias parameter training\n",
    "task_type=\"CAUSAL_LM\": For text generation tasks\n",
    "\n",
    "\n",
    "Applies LoRA to the base model\n",
    "Prints statistics about trainable parameters (showing the efficiency of LoRA)\n",
    "Sets up training configuration via TrainingArguments:\n",
    "\n",
    "Output directories for model checkpoints and logs\n",
    "Training for 50 epochs\n",
    "Small batch size (1) with gradient accumulation (4) for effective batch size of 4\n",
    "Uses paged 32-bit AdamW optimizer\n",
    "Learning rate of 4e-5 with cosine scheduler and 3% warmup\n",
    "Weight decay of 0.001 for regularization\n",
    "Gradient clipping at 0.3 to prevent exploding gradients\n",
    "Enabled mixed precision (FP16) for efficiency\n",
    "TensorBoard integration for training monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cdbc60-e32c-4998-b456-18bdefa45ab2",
   "metadata": {},
   "source": [
    "# Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d43b7-d2cb-452f-81a7-694d9650f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFT Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 137/137 [00:00<00:00, 8862.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "Number of epochs: 50\n",
      "Learning rate: 4e-05\n",
      "Batch size: 1\n",
      "Gradient accumulation steps: 4\n",
      "Effective batch size: 4\n",
      "FP16 enabled: True\n",
      "Trainer ready to start training!\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='1700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  75/1700 02:02 < 45:32, 0.59 it/s, Epoch 2.12/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.574300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.929400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.774300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize SFT Trainer\n",
    "print(\"Initializing SFT Trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"FP16 enabled: {training_args.fp16}\")\n",
    "print(f\"Trainer ready to start training!\")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "output_dir = \"./rohith_llama3_finetuned\"\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "llama_tokenizer.save_pretrained(output_dir)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b65aa-c144-4117-ba87-32a09bd71d3a",
   "metadata": {},
   "source": [
    "This cell:\n",
    "\n",
    "Initializes the Supervised Fine-Tuning (SFT) Trainer\n",
    "Connects the model, dataset, and training arguments\n",
    "Prints a summary of key training parameters\n",
    "Executes the training process for the specified number of epochs (50)\n",
    "Creates an output directory for the final model\n",
    "Saves the fine-tuned model with its LoRA adapters\n",
    "Saves the tokenizer in the same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77c0ee-a80a-405b-9b9d-f8781106d8a6",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50872b02-8357-47b0-a0c9-89a5a97810cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define model names\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "output_dir = \"./rohith_llama3_finetuned\"  # Directory where you saved the fine-tuned model\n",
    "\n",
    "# Load the original base model\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load and merge the LoRA weights with the base model\n",
    "print(\"Loading and merging LoRA weights...\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.config.use_cache = True  # Enable caching for inference\n",
    "\n",
    "# Create a text generation pipeline\n",
    "print(\"Creating text generation pipeline...\")\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=llama_tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0.1,\n",
    "    top_p=0.3,\n",
    "    repetition_penalty=1.2,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"Detail Rohith's GPU-accelerated document processing pipeline\",\n",
    "    \"Explain Rohith's technical documentation work at Radian\",\n",
    "    \"What programming languages does Rohith know\",\n",
    "    \"What is Rohith's educational background?\",\n",
    "    \"What certifications does Rohith have\"\n",
    "]\n",
    "\n",
    "# Generate and print responses\n",
    "for query in test_queries:\n",
    "    print(f\"\\n\\n===== Query: {query} =====\")\n",
    "    test_prompt = f\"<s>[INST] {query} [/INST]\"\n",
    "    output = text_pipeline(test_prompt)\n",
    "    print(\"Model response:\")\n",
    "    print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26789664-0ce3-4de8-a963-bba28f26cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "This cell:\n",
    "\n",
    "Loads a fresh copy of the base model for inference\n",
    "Loads the LoRA adapters and merges them into the base model\n",
    "Enables KV caching for faster inference\n",
    "Creates a text generation pipeline with settings:\n",
    "\n",
    "Maximum length of1024 tokens\n",
    "Temperature of 0.1 \n",
    "Top-p of 0.9 (nucleus sampling)\n",
    "Repetition penalty of 1.2 to reduce repeating text\n",
    "\n",
    "\n",
    "Tests the model with some queries\n",
    "Prints the model's response\n",
    "Confirms the successful completion of the fine-tuning process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
